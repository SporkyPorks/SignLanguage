# detect_signs.py - Streamlit Version with ML Model
import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
import time
import pickle
import math
from collections import deque

# Parameters
BOX_W, BOX_H = 700, 700
PREDICTION_HOLD_TIME = 0.5

# MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_face_mesh = mp.solutions.face_mesh

# Global variables
model = None
pinky_history = deque(maxlen=10)
j_detected_time = None
last_prediction = None
prediction_start_time = None
confirmed_sign = ""
gesture_history = deque(maxlen=50)

# Chin tracking variables
chin_touch_time = None
chin_touch_y = None
thankyou_cooldown = 0

def load_model():
    """Load the trained ASL model"""
    global model
    try:
        with open('trained_asl_model.pkl', 'rb') as f:
            model = pickle.load(f)
        return True
    except FileNotFoundError:
        st.error("‚ùå trained_asl_model.pkl not found. Please ensure the model file is in the same directory.")
        return False
    except Exception as e:
        st.error(f"‚ùå Error loading model: {str(e)}")
        return False

def landmarks_to_features(landmarks):
    """Convert hand landmarks to feature vector"""
    base_x = landmarks[0].x
    base_y = landmarks[0].y
    features = []
    for lm in landmarks:
        features.append(lm.x - base_x)
        features.append(lm.y - base_y)
    return np.array(features)

def predict_sign(model, landmarks):
    """Predict ASL sign using the trained model"""
    if model is None:
        return None
    features = landmarks_to_features(landmarks).reshape(1, -1)
    prediction = model.predict(features)
    return prediction[0]

def is_j_motion(pinky_history):
    """Detect J motion for letter J"""
    if len(pinky_history) < 10:
        return False

    points = np.array(pinky_history)
    deltas = np.diff(points, axis=0)
    
    angles = []
    for i in range(len(deltas) - 1):
        v1 = deltas[i]
        v2 = deltas[i + 1]
        v1_norm = v1 / (np.linalg.norm(v1) + 1e-6)
        v2_norm = v2 / (np.linalg.norm(v2) + 1e-6)
        dot = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)
        angle = np.arccos(dot)
        angles.append(angle)

    total_angle = np.sum(angles)
    dx = points[-1][0] - points[0][0]
    dy = points[-1][1] - points[0][1]

    return dx < -20 and dy > 20 and total_angle > 1.0

def hand_in_box(finger_tips, box_x, box_y, box_w, box_h):
    """Check if all fingertips are within the detection box"""
    return all(box_x <= x <= box_x + box_w and box_y <= y <= box_y + box_h for x, y in finger_tips)

# Color map for letters
COLOR_MAP = {
    "A": "#FF6B6B", "B": "#4ECDC4", "C": "#45B7D1", "D": "#96CEB4", "E": "#FFEAA7",
    "F": "#DDA0DD", "G": "#98D8C8", "H": "#F7DC6F", "I": "#BB8FCE", "J": "#85C1E9",
    "K": "#F8C471", "L": "#82E0AA", "M": "#F1948A", "N": "#85C1E9", "O": "#F8D7DA",
    "P": "#D5DBDB", "Q": "#A9DFBF", "R": "#F9E79F", "S": "#D7BDE2", "T": "#A3E4D7",
    "U": "#FAD7A0", "V": "#FADBD8", "W": "#D6EAF8", "X": "#E8DAEF", "Y": "#FCF3CF", 
    "Z": "#EDEDED", "Thank you": "#2196F3"
}

def run_detection():
    """Main detection function"""
    global model, pinky_history, j_detected_time, last_prediction, prediction_start_time, confirmed_sign
    global chin_touch_time, chin_touch_y, thankyou_cooldown, gesture_history
    
    # Load model if not already loaded
    if model is None:
        if not load_model():
            return
    
    # Create placeholders
    frame_placeholder = st.empty()
    status_placeholder = st.empty()
    detection_placeholder = st.empty()
    
    try:
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            st.error("‚ùå Camera not detected. Please check your webcam connection.")
            return
        
        status_placeholder.success("‚úÖ Camera active - Make ASL letters within the green box")
        
        with mp_hands.Hands(
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        ) as hands, \
        mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        ) as face_mesh:
            
            while st.session_state.get('detection_active', True):
                ret, frame = cap.read()
                if not ret:
                    break

                frame = cv2.flip(frame, 1)
                frame_h, frame_w, _ = frame.shape
                box_x = (frame_w - BOX_W) // 2
                box_y = (frame_h - BOX_H) // 2
                
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Process hands and face
                results = hands.process(rgb_frame)
                face_results = face_mesh.process(rgb_frame)
                
                raw_prediction = None
                
                # Face/chin tracking for "Thank you"
                chin_point = None
                if face_results.multi_face_landmarks:
                    face_landmarks = face_results.multi_face_landmarks[0]
                    chin_lm = face_landmarks.landmark[152]
                    chin_x = int(chin_lm.x * frame_w)
                    chin_y = int(chin_lm.y * frame_h)
                    chin_point = (chin_x, chin_y)
                    cv2.circle(frame, chin_point, 5, (255, 0, 0), -1)

                # Process hands
                if results.multi_hand_landmarks:
                    hand_landmarks = results.multi_hand_landmarks[0]
                    
                    # Track pinky for J motion
                    pinky_tip = hand_landmarks.landmark[20]
                    pinky_x = int(pinky_tip.x * frame_w)
                    pinky_y = int(pinky_tip.y * frame_h)
                    pinky_history.append((pinky_x, pinky_y))
                    
                    # Draw hand landmarks
                    mp_drawing.draw_landmarks(
                        frame, hand_landmarks, mp_hands.HAND_CONNECTIONS
                    )
                    
                    # Get finger tips
                    finger_tips = [(int(hand_landmarks.landmark[i].x * frame_w),
                                   int(hand_landmarks.landmark[i].y * frame_h)) 
                                  for i in [4, 8, 12, 16, 20]]
                    
                    # Check if hand is inside detection box
                    inside_box = hand_in_box(finger_tips, box_x, box_y, BOX_W, BOX_H)
                    
                    if inside_box:
                        # Get ML prediction
                        predicted = predict_sign(model, hand_landmarks.landmark)
                        
                        if predicted:
                            # Check for J motion
                            if predicted == 'I' and is_j_motion(pinky_history):
                                raw_prediction = "J"
                                j_detected_time = time.time()
                            else:
                                raw_prediction = predicted
                    
                    # Check for chin touch (Thank you gesture)
                    if chin_point:
                        middle_finger_tip = hand_landmarks.landmark[12]
                        palm_x = int(middle_finger_tip.x * frame_w)
                        palm_y = int(middle_finger_tip.y * frame_h)
                        
                        cv2.circle(frame, (palm_x, palm_y), 10, (0, 255, 0), -1)
                        
                        dist = math.hypot(palm_x - chin_point[0], palm_y - chin_point[1])
                        current_time = time.time()
                        
                        # Touch detected
                        if dist < 60:
                            chin_touch_time = current_time
                            chin_touch_y = palm_y
                        # Check for downward motion after touch
                        elif chin_touch_time and (current_time - chin_touch_time < 1.0):
                            if palm_y - chin_touch_y > 40 and (current_time - thankyou_cooldown > 2):
                                raw_prediction = "Thank you"
                                thankyou_cooldown = current_time
                                chin_touch_time = None
                                chin_touch_y = None

                # Stability logic for predictions
                current_time = time.time()
                if raw_prediction == last_prediction:
                    if prediction_start_time is None:
                        prediction_start_time = current_time
                    elif current_time - prediction_start_time >= PREDICTION_HOLD_TIME:
                        if confirmed_sign != raw_prediction:
                            confirmed_sign = raw_prediction
                            gesture_history.append(confirmed_sign)
                else:
                    prediction_start_time = current_time
                    last_prediction = raw_prediction

                # Draw detection box
                overlay = frame.copy()
                alpha = 0.15
                cv2.rectangle(overlay, (box_x, box_y), (box_x + BOX_W, box_y + BOX_H), (0, 255, 0), -1)
                cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)
                
                box_color = (0, 255, 0) if confirmed_sign else (0, 0, 255)
                cv2.rectangle(frame, (box_x, box_y), (box_x + BOX_W, box_y + BOX_H), box_color, 3)

                # Display current detection
                if confirmed_sign:
                    cv2.putText(frame, f"Detected: {confirmed_sign}", (30, 60),
                               cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), 4)

                # Show J detection indicator
                if j_detected_time and (time.time() - j_detected_time < 3):
                    cv2.putText(frame, "J detected", (30, 120),
                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)
                elif j_detected_time:
                    j_detected_time = None

                # Update display
                frame_placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), channels="RGB")
                
                # Update detection history
                if gesture_history:
                    recent_signs = list(gesture_history)[-10:][::-1]
                    detection_text = "Recent detections:\n"
                    for sign in recent_signs:
                        detection_text += f"‚Ä¢ {sign}\n"
                    detection_placeholder.text(detection_text)

                time.sleep(0.1)

        cap.release()
    
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")

# Streamlit App
st.set_page_config(
    page_title="ASL Sign Detector",
    layout="wide"
)

st.title("ü§ü ASL Sign Detector")
st.markdown("Real-time ASL alphabet recognition using machine learning")

# Sidebar controls
with st.sidebar:
    st.header("Controls")
    
    if st.button("üé• Start Detection", type="primary"):
        st.session_state.detection_active = True
    
    if st.button("‚èπÔ∏è Stop Detection"):
        st.session_state.detection_active = False
    
    if st.button("üóëÔ∏è Clear History"):
        gesture_history.clear()
        st.success("History cleared!")
    
    st.markdown("---")
    st.subheader("Supported Signs")
    st.markdown("‚Ä¢ A-Z (using ML model)")
    st.markdown("‚Ä¢ Thank you (chin gesture)")
    
    st.markdown("---")
    st.subheader("Instructions")
    st.markdown("""
    1. Click 'Start Detection'
    2. Position hand in green box
    3. Make clear ASL letters
    4. For 'J': Make 'I' then curve down
    5. For 'Thank you': Touch chin, move down
    """)

# Main content
col1, col2 = st.columns([3, 1])

with col1:
    st.subheader("üìπ Camera Feed")
    if st.session_state.get('detection_active', False):
        run_detection()
    else:
        st.info("Click 'Start Detection' to begin")
        st.image("https://via.placeholder.com/640x480/f0f0f0/808080?text=Camera+Feed", caption="Camera will appear here")

with col2:
    st.subheader("üî§ Detected Signs")
    if gesture_history:
        for sign in list(gesture_history)[-5:][::-1]:
            color = COLOR_MAP.get(sign, "#FFFFFF")
            st.markdown(f"""
            <div style='background-color: {color}; color: black; 
                    padding: 8px; border-radius: 5px; margin: 3px 0; 
                    text-align: center; font-weight: bold;'>
                {sign}
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("Detected signs will appear here")

# Initialize session state
if 'detection_active' not in st.session_state:
    st.session_state.detection_active = False
